# Metabolomik Seminar

## Initialization with example data
```{r}
setwd('./metabolomics-in-r')

dat1 <- read.csv("data/xcmsresults.csv", header=TRUE, row.names=1)
# View(dat1)
dat1[1:6,1:4]  # look at the first few rows
```

```{r}
apply(dat1,2, function(x) table(x==0))
```

```{r}
sumstats <- function(z) {
   Mean <- apply(z, 1, mean)
   Median <- apply(z, 1, median)
   SD <- apply(z, 1, sd)
   SE <- apply(z, 1, function(x) sd(x)/sqrt(length(x)))
   CV <- apply(z, 1, function(x) sd(x)/mean(x))
   result <- data.frame(Mean, Median, SD, SE, CV)
   return(result)
}

descstat<-sumstats(dat1)
head(descstat)
```

```{r}
# Number of missing peaks per row (value == 0)
zeroval <- apply(dat1,1,function(x)sum(x == 0))

# number of rows with at least one missing peak
length(which(zeroval > 0))

# number of rows with at least 10 missing peaks
length(which(zeroval >= 10))

# number of zero values in the peak table
sum(zeroval)

# Fraction of missing values in data matrix
round(sum(zeroval) / (nrow(dat1) * ncol(dat1)),3)

###########################################
# 4.1. Imputation of missing values
###########################################

# Replace zeroes with a small value
replacezero <- function(x) "[<-"(x, !x | is.na(x), min(x[x > 0],na.rm = TRUE) / 2)

# Apply function across rows
dat2 <- as.data.frame(t(apply(dat1, 1, replacezero)))
dat2[1:6,1:4]  # look at the first few rows

##########################################
# 4.2. Log transformation
##########################################

# Log transform the data (base 2 log)
logdata <- log(dat2, 2)

############################################
# 4.3. Normalization and scaling
############################################

# Function for pareto scaling
paretoscale <- function(z) {
  rowmean <- apply(z, 1, mean) # row means
  rowsd <- apply(z, 1, sd)  # row standard deviation
  rowsqrtsd <- sqrt(rowsd) # sqrt of sd
  rv <- sweep(z, 1, rowmean,"-")  # mean center
  rv <- sweep(rv, 1, rowsqrtsd, "/")  # dividing by sqrtsd
  return(rv)
}

# Pareto scale log transformed data
logdata.pareto <- paretoscale(logdata)

##############################################
# 5. Principal component analysis
##############################################

# Run PCA (note use of "t" to transpose matrix)
pca <- prcomp(t(logdata.pareto), center=F, scale=F)

# wenn mans einfach macht
plot(pca)
biplot(pca)


# Create a container called "results" for PCA results
results <- summary(pca)

names(results)

# Make a simple scores plot
plot(pca$x[,1], pca$x[,2], type='p', cex=0, pch=20, main="Scores Plot", xlab="PC1", ylab="PC2")
text(pca$x[,1], pca$x[,2], labels=rownames(pca$x), cex=1.0)
abline(h=0, v=0, col="red")

# Make a simple loadings plot (variance among variables)
plot(pca$rotation[,1], pca$rotation[,2], type='p', cex=0.5, pch=20,main="Loadings Plot", xlab="PC1", ylab="PC2")
abline(h=0, v=0, col="red")


# Extract PCA results into data frames

scree.data <- as.data.frame(results$importance)
score.data <- as.data.frame(results$x)
loadings.data <- as.data.frame(results$rotation)

plot(loadings.data$PC1, loadings.data$PC2)
abline(v=0.09, col="red")
abline(v=-0.09, col="red")
abline(h=0.09, col="red")
abline(h=-0.09, col="red")

# Make a new data frame with PC1, PC2, and PC3 loadings
loadings.PC1.PC2 <- loadings.data[,1:3]
loadings.PC1.PC2[1:6,1:3]  # look at the first few rows

# subset significant loadings
loadings.sig <- subset(loadings.PC1.PC2,
                       PC1 > 0.09 | PC1 < -0.09 |
                         PC2 > 0.09 | PC2 < -0.09)

# sanity check - plot the results
plot(loadings.sig$PC1, loadings.sig$PC2)
```

```{r}
####################################################
# Heatmaps
####################################################

library(gplots)
heatmap.2(as.matrix(logdata.pareto))


####################################################
# Boxplots
####################################################

colnames(logdata.pareto)
boxplot(logdata.pareto$HG1)
dat3<-t(logdata.pareto)
rownames(dat3)
colnames(dat3)
nrow(dat3)

boxplot(dat3[,"495.2/2285"])

# Create "Group" vector
Group <- c(rep("hp-2dg Green", 8),
           rep("Manapal Green", 10),
           rep("hp-2dg Red", 10),
           rep("Manapal Red", 10))

boxplot(dat3[,"495.2/2285"]~Group)


par(mfrow=c(2,2))
boxplot(dat3[,"495.2/2285"]~Group,main="495.2/2285")
boxplot(dat3[,"529.8/992"]~Group,main="529.8/992")
boxplot(dat3[,"805.2/2198"]~Group,main="805.2/2198")
boxplot(dat3[,"1136.4/2038"]~Group,main="1136.4/2038")
par(mfrow=c(1,1))



####################################################
# Volcano plots
####################################################

# was ist apply?
apply(dat3,1,sum)
apply(dat3,1,function (x) mean(x)/sd(x))


# Wie liegen die Daten vor?
dat3[1:18,1]
Group
Group[1:18]

# t.test f�r die erste Spalte und die ersten beiden Gruppen
boxplot(dat3[1:18,1]~Group[1:18])
t.test(dat3[1:18,1]~Group[1:18])

# Ergebnis in ein Objekt schreiben
test<-t.test(dat3[1:18,1]~Group[1:18])

# Auf das Ergebnis zugreifen
names(test)
test$p.value
test$estimate
test$estimate[1]
test$estimate[2]
test$estimate[1]-test$estimate[2]
# Fold change
test$estimate[1]/test$estimate[2]
# log2 Fold change
-log2(abs(test$estimate[1]/test$estimate[2]))

######################################################
# F�r alle Metabolite
######################################################

# Sum normalization function - apply to each column
sumnorm <- function(z) {
  colsum <- apply(z, 2, sum)
  rv <- sweep(z, 2, colsum, "/")
  return(rv)
}

# Sum normalize data
set1.norm <- sumnorm(dat2)

# Log transform data
logd1.norm = log2(set1.norm)

# Perform t-test to get p-values 
pvalue <- apply(logd1.norm, 1, function(x) { t.test(x[1:8], x[9:18])$p.value } )

# Apply Benjamini-Hochberg correction (FDR)
p.BHcorr <- p.adjust(pvalue, method = "BH")

# Calculate negative log of FDR-adjusted p value
p.BH.nlog <- -log10(p.BHcorr)

# Calculate row-wise group means for sum normalized data
hp <- apply(set1.norm[1:8], 1, FUN=mean)
Man <- apply(set1.norm[9:18], 1, FUN=mean)

# Calculate log2 Fold Change from group means
FC <- hp/Man
log2FC <- log(FC,2)

# Make new data frame with volcano plot data
volcano.dat1 <- data.frame(hp, Man, pvalue, p.BHcorr, p.BH.nlog, FC, log2FC)

# Sort in ascending order by FDR
volcano.dat1 <- volcano.dat1[order(p.BHcorr),]

# make a simple volcano plot
plot(volcano.dat1$log2FC, volcano.dat1$p.BH.nlog)

# add cutoff lines to show significant variables
abline(v=2, col="red")
abline(v=-2, col="red")
abline(h=2, col="red")
```